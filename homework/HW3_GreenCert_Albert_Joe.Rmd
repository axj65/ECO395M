---
title: "HW3: Green Certification"
output: "md_document"
date: "2023-03-23"
---

```{r setup, include=FALSE}

#Set Directory
knitr::opts_chunk$set(echo = FALSE, include = TRUE)
knitr::opts_knit$set(root.dir = "/Users/albertjoe33/Documents/UT_Austin/Stat_Learning/ECO395M/homework/")

#Load Libraries
library(readr)
library(tidyverse)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(dplyr)
library(rmarkdown)
library(lubridate)
library(mosaic)
library(gamlr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(pdp)
library(lattice)

#Load Dataset
green <- read_csv("Data/greenbuildings.csv")

```

## 3. Predictive Model Building: Green Certification

### Introduction

The goal of this exercise is to build the best predictive model possible for 'revenue per square foot per calendar year' and to use this model to quantify the average change in rental income per square foot associated with 'green certification', holding other features of the building constant. In this exercise, I build three models:a linear model, a random forest model, a gradient-boosted tree model, and a stacked model that combines the three models. I then create a partial dependence plot of the stacked model to quantify the average change in rental income associated with 'green certification'. 

### Methods

Although not shown in the report, worthy of note is that I performed the following analysis using 5 different random seeds for all models. The results were consistent through all random seeds. 

#### Data Preparation 

I partitioned the dataset into a training set and a test set using an 80-20 split. For all models, the training set is used for training the model, hyperparameter tuning, and model selection. In other words, k-fold cross validation for the linear model and the gradient-boosted tree model are done on the training set. The coefficients for the stacked model are also evaluated on the training set. The test set is reserved to evaluate the models' final performance on data that it has not yet seen.

For all models, I designated 'revenue' (the product of 'Rent' and 'leasing_rate) as the response variable. I then removed 'Rent' and 'leasing_rate' from the model. I removed 'LEED' and 'Energystar' from the models as I am evaluating 'green_certification' (a building is green_certified if it has either a 'LEED' or 'Energystar' certification). Also removed from all models are the "CS_PorpertyID" and "total_dd_07" to reduce noise. 

There were also 225 observations that I removed doe either having NaN values or having a leasing_rate of 0. This is because these observations are likely due to certain buildings undergoing renovations or other unknown factors, and they likely add unnecessary noise to the data. 

```{r chunk1}

set.seed(9456)

#Drop rows where the leasing_rate is 0 (does not make sense to keep an empty building so maybe the building is under rennovation or something)
green <- green %>% filter(leasing_rate != 0)

#Add the variable of interest
green <- green %>% mutate(revenue = Rent * (leasing_rate/100))

#Drop NaN
green <- drop_na(green)

#Hold out testing data for final validation of model
green_split = initial_split(green, prop = 0.8)
green_train = training(green_split)
green_test = testing(green_split)
```

#### Stepwise Linear Model

I first wanted to see the effect of 'green_rating'(the variable name for 'green_certification') on 'revenue' in a linear model as this is generally the most interpretable. 

For the linear model, I performed step-wise selection. I removed 'cluster' as there were too many clusters to treat this as a categorical variable. I also removed 'green_rating' for the initial training of the step-wise model. This is because this model takes into account all feature variables and their interactions. Once the model was trained, I added 'green_rating' back into the model without interactions in order to leave the effect of 'green_rating' on 'revenue' more interpretable. 


```{r chunk2, include=FALSE}
#Make initial lm and train stepwise model
green_lm <- lm(revenue ~ . -LEED -Energystar -CS_PropertyID -Rent -leasing_rate -total_dd_07 -green_rating -cluster, data=green_train)
green_step <- step(green_lm, scope=~(.)^2)
```


```{r chunk3, include=FALSE}
getCall(green_step)
```

Once the linear model was trained and selected, I added 'green_rating' back into the linear model and performed K-fold cross using 5 folds. The following show the RMSEs for both the training and testing sets. Note that the RMSE for both the training and testing sets are approximately equal (this was consistent over 5 random seeds).

```{r chunk4}
set.seed(9456)

#Calculate RSME using stepwise function with 2 interactions

K_folds = 5
green_folds = green_train %>%
  mutate(fold_id = rep(1:K_folds, length=nrow(green_train)) %>% sample)

#RMSE of Medium Model
rmse_lm = foreach(fold = 1:K_folds, .combine = 'c') %do% {
  step_lm <- lm(revenue ~ green_rating + size + empl_gr + stories + age + renovated + 
    class_a + class_b + net + amenities + cd_total_07 + hd_total07 + 
    Precipitation + Gas_Costs + Electricity_Costs + City_Market_Rent + 
    size:City_Market_Rent + stories:City_Market_Rent + size:Precipitation + 
    Electricity_Costs:City_Market_Rent + stories:class_a + cd_total_07:hd_total07 + 
    amenities:Gas_Costs + amenities:Precipitation + renovated:Precipitation + 
    class_a:amenities + size:stories + size:age + empl_gr:Electricity_Costs + 
    size:renovated + stories:renovated + age:class_b + stories:Gas_Costs + 
    class_a:Electricity_Costs + age:City_Market_Rent + age:Electricity_Costs + 
    renovated:City_Market_Rent + renovated:hd_total07 + size:Electricity_Costs + 
    renovated:Gas_Costs + size:hd_total07 + hd_total07:Precipitation + 
    cd_total_07:Precipitation + stories:class_b + class_a:hd_total07 + 
    Precipitation:City_Market_Rent, 
    data=filter(green_folds, fold_id!=fold))
  modelr::rmse(step_lm, data = filter(green_folds, fold_id == fold))
}

#Actual step model
green_lm <- lm(revenue ~ green_rating + size + empl_gr + stories + age + renovated + 
    class_a + class_b + net + amenities + cd_total_07 + hd_total07 + 
    Precipitation + Gas_Costs + Electricity_Costs + City_Market_Rent + 
    size:City_Market_Rent + stories:City_Market_Rent + size:Precipitation + 
    Electricity_Costs:City_Market_Rent + stories:class_a + cd_total_07:hd_total07 + 
    amenities:Gas_Costs + amenities:Precipitation + renovated:Precipitation + 
    class_a:amenities + size:stories + size:age + empl_gr:Electricity_Costs + 
    size:renovated + stories:renovated + age:class_b + stories:Gas_Costs + 
    class_a:Electricity_Costs + age:City_Market_Rent + age:Electricity_Costs + 
    renovated:City_Market_Rent + renovated:hd_total07 + size:Electricity_Costs + 
    renovated:Gas_Costs + size:hd_total07 + hd_total07:Precipitation + 
    cd_total_07:Precipitation + stories:class_b + class_a:hd_total07 + 
    Precipitation:City_Market_Rent, 
    data = green_train)

#Training RMSE with 5 folds
rmse_train_lm <- mean(rmse_lm) %>% round(4)
cat("Linear Model in-sample mean RMSE of 5 folds:", rmse_train_lm, "\n")

#RMSE on Test Set
rmse_test_lm <- rmse(green_lm, green_test) %>% round(4)
cat("Linear Model out-of-sample RMSE:", rmse_test_lm, "\n")

```

The following shows the summary statistics for 'green_rating'. This model shows that holding all other variables constant, having a 'green_certification' increases the 'revenue per square foot per calendar year' by about \$1.32. 

```{r chunk5}
summary(green_lm)$coefficients['green_rating',] %>% round(4)
```

#### Random Forest Model

For this model, cross-validation was not conducted on the training set. However, the RMSE results were consistent across 5 random seeds.The following show the RMSEs of the Random Forest Model using 500 trees as performance did not improve by adding more trees. 

```{r chunk6}
set.seed(9456)

#Random Forest Model
green_forest <- randomForest(revenue ~ . -LEED -Energystar -CS_PropertyID -Rent -leasing_rate -total_dd_07, 
                             data=green_train, importance = TRUE, ntree=500)

#RMSE of 
rmse_train_rf <- modelr::rmse(green_forest, green_train)
rmse_test_rf <- modelr::rmse(green_forest, green_test)

cat("Random Forest Model in-sample RMSE:", rmse_train_rf, "\n")
cat("Random Forest Model out-of-sample RMSE:", rmse_test_rf, "\n")
```


#### Gradient-boosted Tree Model

The Gradient-boosted Tree Model was trained using 5 cross validated folds with an interaction depth of 13, a shrinkage of 0.3, and 1,000 trees. Although the RMSE of the test set was smaller than that of the Random Forest Model and the RMSEs were fairly consistent, it is worthy to note that the Random Forest Model outperformed this model across 3 of 5 random seeds. The following show the RMSEs of the Gradient-boosted Tree Model. 

```{r chunk7, include=FALSE}
set.seed(9456)

#Gradient-boosted Tree
green_boost = gbm(revenue ~ . -LEED -Energystar -CS_PropertyID -Rent -leasing_rate -total_dd_07, data=green_train, 
               interaction.depth=13, n.trees=1000, shrinkage=.3, distribution = 'gaussian', cv.folds=5)

rmse_train_gbm <- modelr::rmse(green_boost, green_train)
rmse_test_gbm <- modelr::rmse(green_boost, green_test)
```

```{r chunk8}
cat("Random Forest Model in-sample RMSE:", rmse_train_gbm, "\n")
cat("Random Forest Model out-of-sample RMSE:", rmse_test_gbm, "\n")
```


#### Stacked Model

Finally, I used a Stacked Model, in which I used an ensemble approach combining the predictions of the three previous models using a Linear Regression model as a second level learner to stack the individual model predictions. I also tried this approach using 5 different Random Forest Models, but the RMSE for the Stacked Model using a Linear Model, a Random Forest Model, and a Gradient-boosted Tree Model consistently gave lower RMSEs. 

```{r chunk9, include=FALSE}

#Generate Predictions on the training data
gbm_pred <- predict(green_boost, newdata = green_train)
forest_pred <- predict(green_forest, newdata = green_train)
lm_pred <- predict(green_lm, newdata = green_train)

#Create a dataframe with predictions from all the models along with the actual revenue
stacked_data <- data.frame(gbm_pred, forest_pred, lm_pred, green_train$revenue)
stacked_data <- stacked_data %>% rename(revenue = green_train.revenue)

#Second level learning using lm
stacked_model <- lm(revenue ~ ., data = stacked_data)

```

The following shows the in-sample RMSE for the Stacked Model. 
```{r chunk10}
#RMSE for stacked model
stacked_train_rmse <- modelr::rmse(stacked_model, stacked_data)
cat("Stacked Model in-sample RMSE:", stacked_train_rmse, "\n")
```

The following shows the coefficients of the second-level learner. Note that these coefficients were all generated using the training set. Not that these are ultimately used to generate predictions for the Stacked Model in the test set.

```{r chunk11}
coef(stacked_model)
```


```{r chunk12, include=FALSE}
#Generate predictions on the test set
gbm_pred2 <- predict(green_boost, newdata = green_test)
forest_pred2 <- predict(green_forest, newdata = green_test)
lm_pred2 <- predict(green_lm, newdata = green_test)

#Create a dataframe with predictions from all the models along with the actual revenue
stacked_data2 <- data.frame(gbm_pred2, forest_pred2, lm_pred2, green_test$revenue)
stacked_data2 <- stacked_data2 %>% rename(revenue = green_test.revenue)

#Generate predictions for the stacked model on the test set using the coefficients learned from the training set
stacked_data2 <- stacked_data2 %>% mutate(pred = -0.04374805 + 0.94938894*gbm_pred2 + 0.12318462*forest_pred2 - 0.07056185 *lm_pred2)


```

The following show the out-of-sample RMSE of the Stacked Model.

```{r chunk13}
#Calculate RMSE
stacked_test_rmse <- sqrt(mean((stacked_data2$revenue - stacked_data2$pred)^2))
cat("Stacked Model out-of-sample RMSE:", stacked_test_rmse, "\n")

```


### Modeling Choice

I chose to use the Stacked Model as my final predictive model. This is because the RMSEs of this model were extremely consistent across 5 different random seeds, and outperformed all previous models across 4 of 5 random seeds. It only slightly unperformed the Random Forest Model on one iteration. Although fairly consistent, the changes in RMSEs for the Random Forest and Gradient-boosted Tree Models could indicate a small amount of overfitting. Consequently, it seemed that by ensembling the three models I initially fit, I improved predictive accuracy and reduced the risk of overfitting. 


#### Partial Dependence of Green Certification

As my model of choice was the stacked model, I also calculated the partial dependence of 'green_certification' using a combination of the partial dependencies from the Linear Model, the Random Forest Model, and the Gradient-boosted Tree Model. I did this by first generating the partial dependencies of each model individually. I then weighted the respective partial dependencies by using the inverse of the RMSE and scaling those weights to that the sum to 1. In other words, the models with lower RMSE have more weight. 


```{r chunk14}

#Calculate partial dependencies

grid <- seq(from=0, to=1)

gbm_pd <- partial(green_boost, pred.var = "green_rating", grid.points = grid, train = green_test, n.trees=1000)

rf_pd <- partial(green_forest, pred.var = "green_rating", grid.points = grid, train = green_test, n.trees=500)

lm_coef <- coef(green_lm)["green_rating"]
lm_pd <- grid * lm_coef

```


```{r chunk15}

#Calculate inverse RMSEs for all models
gbm_inverse_rmse <- rmse_test_gbm ^ -1
rf_inverse_rmse <- rmse_test_rf ^ -1
lm_inverse_rmse <- rmse_test_lm ^ -1

total_inverse <- gbm_inverse_rmse + rf_inverse_rmse + lm_inverse_rmse

#Weight partial dependencies by inverse RMSE
gbm_weight <- gbm_inverse_rmse / total_inverse
rf_weight <- rf_inverse_rmse / total_inverse
lm_weight <- lm_inverse_rmse / total_inverse

#Calculate the Stacked Model Partial Dependency
stacked_pd <- gbm_weight*gbm_pd$yhat + rf_weight*rf_pd$yhat + lm_weight*lm_pd

```


```{r chunk16}
#Create Dataframe with the stacked partial dependency, makes easier to plot
df <- data.frame(grid,stacked_pd)
df <- df %>% rename(green_certification = grid)
df <- df %>% rename(revenue = stacked_pd)
df <- df %>% mutate(revenue = round(revenue, 3))
```

The following show the partial dependence of 'green_certification', and plots it. Note this shows that holding all other variables constant, having a 'green_certification' increases the 'revenue per square foot per calendar year' by about \$0.61.

```{r chunk17}
green_cert_pd <- df$revenue[2] - df$revenue[1]
cat("Partial Dependece of Green Certification:", green_cert_pd, "\n")
```

```{r chunk18}
#Plot Partial Dependency
ggplot(df, aes(x=green_certification, y=revenue)) + 
  geom_line() + 
  scale_y_continuous(
    limits = range(df$revenue),       
    breaks = range(df$revenue),      
    labels = range(df$revenue) 
  )
```

### Conclusion

The Stacked Model likely performed the best because it leverages the strengths of the other models and minimizes their weaknesses. For example, the Linear Model showed that holding all other variables constant, having a 'green_certification' increases the 'revenue per square foot per calendar year' by about \$1.32. Although this seems like a small number, it is actually ver significant. If a building has about 50 rooms averaging about 1,000 square feet, the difference in 'revenue' is approximately $66,000. There could be other factors not captured in the data that a linear model can account for that could contribute to this difference. In other words, we would need more data to train a better linear model as it is extremely difficult for a linear model to capture nuances based on the current dataset.

The Random Forest and Gradient-boosted Tree Models are better able to capture the nuances in the data. However, this is done at the risk of overfitting. Consequently, by creating a Stacked Model, the risk of overfitting is reduced. The partial dependence of the Stacked Model showed that holding all other variables constant, having a 'green_certification' increases the 'revenue per square foot per calendar year' by about \$0.57. If a building has about 50 rooms averaging about 1,000 square feet, the difference in 'revenue' is now approximately $28,500. 

Although much improved from the Linear Model, the RMSE of the Stacked Model is still pretty high at $7.205. More accurate predictions and a more accurate partial dependency of 'green_certification' can be achieved with more relevant data. This dataset contains data on commercial rental properties from across the United States, but does not even have the city and state in which these properties are located. Although the City_Market_Rent variable may capture some of this, even different locations within a city could have significantly different revenues. 

Overall, it seems like having a Green Certification does seem to have a positive effect on revenue, but based on the current amount of data, I cannot confidently conclude that it is a \$0.57 increase in 'revenue per square foot per calendar year' or that this relationship is causal.


