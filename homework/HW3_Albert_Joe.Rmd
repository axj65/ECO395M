---
title: "HW3"
output: "md_document"
date: "2023-03-23"
---

```{r setup, include=FALSE}

#Set Directory
knitr::opts_chunk$set(echo = FALSE, include = TRUE)
knitr::opts_knit$set(root.dir = "/Users/albertjoe33/Documents/UT_Austin/Stat_Learning/ECO395M/homework/")

#Load Libraries
library(readr)
library(tidyverse)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(dplyr)
library(rmarkdown)
library(lubridate)
library(mosaic)
library(gamlr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(pdp)
library(lattice)

#Load Datasets
housing <- read_csv("Data/CAhousing.csv")
dengue <- read_csv("Data/dengue.csv")
green <- read_csv("Data/greenbuildings.csv")

```

## 1. What Causes What?

1. All cities are different, and the circumstances of an increased number of matter in terms of answering the question of how more cops affect street crime. Just because more police lowers crime rate in one city does not mean that it does so in another. So aggregating different cities could confound the results. Furthermore, increasing police because of expected higher crime rates might confound the results as well (it may even show that more police correlate with higher crime). 

2. The researchers were able to isolate this affect by using a natural experiment. They found no relationship between high terror alert days and crime. On high-alert days, there were more police because not because of crime so the researchers could isolate the affect of police number on crime. The table basically shows that if multiple samples were drawn from the same population, we would expect the true population value of the coefficients to be found within 95% of the samples (99% for the 1% level).  

3. They wanted to ensure that the effect of metro ridership (a proxy for how many people are out and about) did not change. This is because if there are significantly less people out on high-alert days, then crime could potentially be lower because less criminals are on the street or could potentially be higher because there are less people able to observe crimes being committed. Either way, that would confound the results of the variable of interest. 

4. The model being estimated here shows the effect of high-alert days (meaning more police) on District 1 and all other districts, controlling for midday ridership. The results show that there is a statistically significant reduction in crime for District 1 on high-alert days but not all other districts (grouped together). I am not sure if District 1 had more police on high-alert days but the other districts did not as this could further support the conclusion that more police reduces crime rates. Or if there were more police in all districts, it could mean indicate that more police may reduce crime rates in some areas but not others. 

<br/>

## 2. Tree Modeling: Dengue Cases

In this section, I use 3 methods (CART, Random Forest, and Gradient-boosted Trees) to predict dengue cases. For each method, I create and display the results of two models. Model_1 for each method uses data that removes all rows with NaN values (removes 214 rows). Model_2 for each method uses data that removes 60 rows with NaN values and also removes the ndvi_nw variable. 

For both models, I hold out 25% of the data as a test set. 


### a. CART

```{r chunk1, include=FALSE}
#Set Seed and Load Datasets
set.seed(9456)

#separate training and  validation data
dengue_split = initial_split(dengue, prop = 0.75)
dengue_train = training(dengue_split)
dengue_test = testing(dengue_split)

#Dataset with all NaN values dropped
dengue2 <- drop_na(dengue)
dengue2_split = initial_split(dengue2, prop = 0.75)
dengue2_train = training(dengue2_split)
dengue2_test = testing(dengue2_split)

#Dataset with some NaN values dropped and all the ndvi variables dropped
dengue3 <- dengue[complete.cases(dengue$precipitation_amt), ]
dengue3 <- dengue3[complete.cases(dengue3$ndvi_nw), ]
dengue3 <- subset(dengue3, select = -c(ndvi_ne))
dengue3_split = initial_split(dengue3, prop = 0.75)
dengue3_train = training(dengue3_split)
dengue3_test = testing(dengue3_split)

```

```{r chunk2}
#Create tree for two models
dengue_tree2 <- rpart(total_cases ~ . , data=dengue2_train, control = rpart.control(cp=0.0001,minsplit = 5, k = 5))
 
dengue_tree3 <- rpart(total_cases ~ ., data=dengue3_train, control = rpart.control(cp=0.0001,minsplit = 5, k = 5))
```

```{r chunk3}

#create function that prunes the tree to 1SE above the one with lowest in sample error
prune_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp=cp_opt)
}

#Prunes tree for both models
tree2_prune <- prune_1se(dengue_tree2)
tree3_prune <- prune_1se(dengue_tree3)
```

The following show the RMSE for the CART method for both models:

```{r chunk4}
cat("CART in-sample RMSE (Model_1):", modelr::rmse(tree2_prune, dengue2_train),  "\n")
cat("CART out-of-sample RMSE (Model_1):", modelr::rmse(tree2_prune, dengue2_test),  "\n")
cat("\n")
cat("CART in-sample RMSE (Model_2):", modelr::rmse(tree3_prune, dengue3_train),  "\n")
cat("CART out-of-sample RMSE (Model_2):", modelr::rmse(tree3_prune, dengue3_test),  "\n")
```

### b. Random Forest

The following show the RMSE of the Random Forest method for both models:

```{r chunk5}
set.seed(9456)

dengue_forest2 <- randomForest(total_cases ~ ., data=dengue2_train, importance = TRUE)

dengue_forest3 <- randomForest(total_cases ~ ., data=dengue3_train, importance = TRUE)

# let's compare RMSE on the test set

cat("Random Forest in-sample RMSE (Model_1):", modelr::rmse(dengue_forest2, dengue2_train), "\n")
cat("Random Forest out-of-sample RMSE (Model_1):", modelr::rmse(dengue_forest2, dengue2_test), "\n")
cat("\n")
cat("Random Forest in-sample RMSE (Model_2):", modelr::rmse(dengue_forest3, dengue3_train), "\n")
cat("Random Forest out-of-sample RMSE (Model_2):",modelr::rmse(dengue_forest3, dengue3_test), "\n")


```


### c. Gradient_boosted Trees

```{r chunk6}
#Factor categorical variables

dengue2_train$city = factor(dengue2_train$city)
dengue2_train$season = factor(dengue2_train$season)
dengue2_test$city = factor(dengue2_test$city)
dengue2_test$season = factor(dengue2_test$season)

dengue3_train$city = factor(dengue3_train$city)
dengue3_train$season = factor(dengue3_train$season)
dengue3_test$city = factor(dengue3_test$city)
dengue3_test$season = factor(dengue3_test$season)
```


The following show the RMSE of the Gradient-boosted Tree method for both models. This method has the lowest RMSEs. Model_2 has the lowest RMSE of all models performed thus far.

```{r chunk7}
set.seed(9456)

#Create Model 1
dengue_boost2 = gbm(total_cases ~ ., data=dengue2_train, 
               interaction.depth=10, n.trees=500, shrinkage=.01, distribution = 'gaussian', cv.folds=5)

# RMSE
cat("Gradient-boosted Tree in-sample RMSE (Model_1):", modelr::rmse(dengue_boost2, dengue2_train), "\n")
cat("\n")
cat("\n")
cat("Gradient-boosted Tree out-of-sample RMSE (Model_1):", modelr::rmse(dengue_boost2, dengue2_test), "\n")

```

<br/>

```{r chunk8}
set.seed(9456)

#Create Model 2
dengue_boost3 = gbm(total_cases ~ ., data=dengue3_train, 
               interaction.depth=10, n.trees=500, shrinkage=.01, distribution = 'gaussian', cv.folds = 5)

# RMSE
cat("Gradient-boosted Tree in-sample RMSE (Model_2):", modelr::rmse(dengue_boost3, dengue3_train), "\n")
cat("\n")
cat("\n")
cat("Gradient-boosted Tree out-of-sample RMSE (Model_2):", modelr::rmse(dengue_boost3, dengue3_test), "\n")


```

### d. Partial Dependence Plots of Gradient_boosted Trees for Model_2

Note that the gbm function used 182 trees above, so the following partial dependence plots have n.trees set to 182.


```{r chunk9}
partial(dengue_boost3, pred.var = "specific_humidity", n.trees=182, plot = TRUE) 
partial(dengue_boost3, pred.var = "precipitation_amt", n.trees=182, plot = TRUE)
partial(dengue_boost3, pred.var = "tdtr_k", n.trees=182, plot = TRUE)
```


## 3. Predictive Model Building: Green Certification

### Introduction

The goal of this exercise is to build the best predictive model possible for 'revenue per square foot per calendar year' and to use this model to quantify the average change in rental income per square foot associated with 'green certification', holding other features of the building constant. In this exercise, I build three models:a linear model, a random forest model, a gradient-boosted tree model, and a stacked model that combines the three models. I then create a partial dependence plot of the stacked model to quantify the average change in rental income associated with 'green certification'. 

### Methods

Although not shown in the report, worthy of note is that I performed the following analysis using 5 different random seeds for all models. The results were consistent through all random seeds. 

#### Data Preparation 

I partitioned the dataset into a training set and a test set using an 80-20 split. For all models, the training set is used for training the model, hyperparameter tuning, and model selection. In other words, k-fold cross validation for the linear model and the gradient-boosted tree model are done on the training set. The coefficients for the stacked model are also evaluated on the training set. The test set is reserved to evaluate the models' final performance on data that it has not yet seen.

For all models, I designated 'revenue' (the product of 'Rent' and 'leasing_rate) as the response variable. I then removed 'Rent' and 'leasing_rate' from the model. I removed 'LEED' and 'Energystar' from the models as I am evaluating 'green_certification' (a building is green_certified if it has either a 'LEED' or 'Energystar' certification). Also removed from all models are the "CS_PorpertyID" and "total_dd_07" to reduce noise. 

There were also 225 observations that I removed doe either having NaN values or having a leasing_rate of 0. This is because these observations are likely due to certain buildings undergoing renovations or other unknown factors, and they likely add unnecessary noise to the data. 

```{r chunk10}

set.seed(9456)

#Drop rows where the leasing_rate is 0 (does not make sense to keep an empty building so maybe the building is under rennovation or something)
green <- green %>% filter(leasing_rate != 0)

#Add the variable of interest
green <- green %>% mutate(revenue = Rent * (leasing_rate/100))

#Drop NaN
green <- drop_na(green)

#Hold out testing data for final validation of model
green_split = initial_split(green, prop = 0.8)
green_train = training(green_split)
green_test = testing(green_split)
```

#### Stepwise Linear Model

I first wanted to see the effect of 'green_rating'(the variable name for 'green_certification') on 'revenue' in a linear model as this is generally the most interpretable. 

For the linear model, I performed step-wise selection. I removed 'cluster' as there were too many clusters to treat this as a categorical variable. I also removed 'green_rating' for the initial training of the step-wise model. This is because this model takes into account all feature variables and their interactions. Once the model was trained, I added 'green_rating' back into the model without interactions in order to leave the effect of 'green_rating' on 'revenue' more interpretable. 


```{r chunk11, include=FALSE}
#Make initial lm and train stepwise model
green_lm <- lm(revenue ~ . -LEED -Energystar -CS_PropertyID -Rent -leasing_rate -total_dd_07 -green_rating -cluster, data=green_train)
green_step <- step(green_lm, scope=~(.)^2)
```


```{r chunk12, include=FALSE}
getCall(green_step)
```

Once the linear model was trained and selected, I added 'green_rating' back into the linear model and performed K-fold cross using 5 folds. The following show the RMSEs for both the training and testing sets. Note that the RMSE for both the training and testing sets are approximately equal (this was consistent over 5 random seeds).

```{r chunk13}
set.seed(9456)

#Calculate RSME using stepwise function with 2 interactions

K_folds = 5
green_folds = green_train %>%
  mutate(fold_id = rep(1:K_folds, length=nrow(green_train)) %>% sample)

#RMSE of Medium Model
rmse_lm = foreach(fold = 1:K_folds, .combine = 'c') %do% {
  step_lm <- lm(revenue ~ green_rating + size + empl_gr + stories + age + renovated + 
    class_a + class_b + net + amenities + cd_total_07 + hd_total07 + 
    Precipitation + Gas_Costs + Electricity_Costs + City_Market_Rent + 
    size:City_Market_Rent + stories:City_Market_Rent + size:Precipitation + 
    Electricity_Costs:City_Market_Rent + stories:class_a + cd_total_07:hd_total07 + 
    amenities:Gas_Costs + amenities:Precipitation + renovated:Precipitation + 
    class_a:amenities + size:stories + size:age + empl_gr:Electricity_Costs + 
    size:renovated + stories:renovated + age:class_b + stories:Gas_Costs + 
    class_a:Electricity_Costs + age:City_Market_Rent + age:Electricity_Costs + 
    renovated:City_Market_Rent + renovated:hd_total07 + size:Electricity_Costs + 
    renovated:Gas_Costs + size:hd_total07 + hd_total07:Precipitation + 
    cd_total_07:Precipitation + stories:class_b + class_a:hd_total07 + 
    Precipitation:City_Market_Rent, 
    data=filter(green_folds, fold_id!=fold))
  modelr::rmse(step_lm, data = filter(green_folds, fold_id == fold))
}

#Actual step model
green_lm <- lm(revenue ~ green_rating + size + empl_gr + stories + age + renovated + 
    class_a + class_b + net + amenities + cd_total_07 + hd_total07 + 
    Precipitation + Gas_Costs + Electricity_Costs + City_Market_Rent + 
    size:City_Market_Rent + stories:City_Market_Rent + size:Precipitation + 
    Electricity_Costs:City_Market_Rent + stories:class_a + cd_total_07:hd_total07 + 
    amenities:Gas_Costs + amenities:Precipitation + renovated:Precipitation + 
    class_a:amenities + size:stories + size:age + empl_gr:Electricity_Costs + 
    size:renovated + stories:renovated + age:class_b + stories:Gas_Costs + 
    class_a:Electricity_Costs + age:City_Market_Rent + age:Electricity_Costs + 
    renovated:City_Market_Rent + renovated:hd_total07 + size:Electricity_Costs + 
    renovated:Gas_Costs + size:hd_total07 + hd_total07:Precipitation + 
    cd_total_07:Precipitation + stories:class_b + class_a:hd_total07 + 
    Precipitation:City_Market_Rent, 
    data = green_train)

#Training RMSE with 5 folds
rmse_train_lm <- mean(rmse_lm) %>% round(4)
cat("Linear Model in-sample mean RMSE of 5 folds:", rmse_train_lm, "\n")

#RMSE on Test Set
rmse_test_lm <- rmse(green_lm, green_test) %>% round(4)
cat("Linear Model out-of-sample RMSE:", rmse_test_lm, "\n")

```

The following shows the summary statistics for 'green_rating'. This model shows that holding all other variables constant, having a 'green_certification' increases the 'revenue per square foot per calendar year' by about \$1.32. 

```{r chunk14}
summary(green_lm)$coefficients['green_rating',] %>% round(4)
```

#### Random Forest Model

For this model, cross-validation was not conducted on the training set. However, the RMSE results were consistent across 5 random seeds.The following show the RMSEs of the Random Forest Model using 500 trees as performance did not improve by adding more trees. 

```{r chunk15}
set.seed(9456)

#Random Forest Model
green_forest <- randomForest(revenue ~ . -LEED -Energystar -CS_PropertyID -Rent -leasing_rate -total_dd_07, 
                             data=green_train, importance = TRUE, ntree=500)

#RMSE of 
rmse_train_rf <- modelr::rmse(green_forest, green_train)
rmse_test_rf <- modelr::rmse(green_forest, green_test)

cat("Random Forest Model in-sample RMSE:", rmse_train_rf, "\n")
cat("Random Forest Model out-of-sample RMSE:", rmse_test_rf, "\n")
```


#### Gradient-boosted Tree Model

The Gradient-boosted Tree Model was trained using 5 cross validated folds with an interaction depth of 13, a shrinkage of 0.3, and 1,000 trees. Although the RMSE of the test set was smaller than that of the Random Forest Model and the RMSEs were fairly consistent, it is worthy to note that the Random Forest Model outperformed this model across 3 of 5 random seeds. The following show the RMSEs of the Gradient-boosted Tree Model. 

```{r chunk16, include=FALSE}
set.seed(9456)

#Gradient-boosted Tree
green_boost = gbm(revenue ~ . -LEED -Energystar -CS_PropertyID -Rent -leasing_rate -total_dd_07, data=green_train, 
               interaction.depth=13, n.trees=1000, shrinkage=.3, distribution = 'gaussian', cv.folds=5)

rmse_train_gbm <- modelr::rmse(green_boost, green_train)
rmse_test_gbm <- modelr::rmse(green_boost, green_test)
```

```{r chunk17}
cat("Random Forest Model in-sample RMSE:", rmse_train_gbm, "\n")
cat("Random Forest Model out-of-sample RMSE:", rmse_test_gbm, "\n")
```


#### Stacked Model

Finally, I used a Stacked Model, in which I used an ensemble approach combining the predictions of the three previous models using a Linear Regression model as a second level learner to stack the individual model predictions. I also tried this approach using 5 different Random Forest Models, but the RMSE for the Stacked Model using a Linear Model, a Random Forest Model, and a Gradient-boosted Tree Model consistently gave lower RMSEs. 

```{r chunk18, include=FALSE}

#Generate Predictions on the training data
gbm_pred <- predict(green_boost, newdata = green_train)
forest_pred <- predict(green_forest, newdata = green_train)
lm_pred <- predict(green_lm, newdata = green_train)

#Create a dataframe with predictions from all the models along with the actual revenue
stacked_data <- data.frame(gbm_pred, forest_pred, lm_pred, green_train$revenue)
stacked_data <- stacked_data %>% rename(revenue = green_train.revenue)

#Second level learning using lm
stacked_model <- lm(revenue ~ ., data = stacked_data)

```

The following shows the in-sample RMSE for the Stacked Model. 
```{r chunk19}
#RMSE for stacked model
stacked_train_rmse <- modelr::rmse(stacked_model, stacked_data)
cat("Stacked Model in-sample RMSE:", stacked_train_rmse, "\n")
```

The following shows the coefficients of the second-level learner. Note that these coefficients were all generated using the training set. Not that these are ultimately used to generate predictions for the Stacked Model in the test set.

```{r chunk20}
coef(stacked_model)
```


```{r chunk21, include=FALSE}
#Generate predictions on the test set
gbm_pred2 <- predict(green_boost, newdata = green_test)
forest_pred2 <- predict(green_forest, newdata = green_test)
lm_pred2 <- predict(green_lm, newdata = green_test)

#Create a dataframe with predictions from all the models along with the actual revenue
stacked_data2 <- data.frame(gbm_pred2, forest_pred2, lm_pred2, green_test$revenue)
stacked_data2 <- stacked_data2 %>% rename(revenue = green_test.revenue)

#Generate predictions for the stacked model on the test set using the coefficients learned from the training set
stacked_data2 <- stacked_data2 %>% mutate(pred = -0.04374805 + 0.94938894*gbm_pred2 + 0.12318462*forest_pred2 - 0.07056185 *lm_pred2)


```

The following show the out-of-sample RMSE of the Stacked Model.

```{r chunk22}
#Calculate RMSE
stacked_test_rmse <- sqrt(mean((stacked_data2$revenue - stacked_data2$pred)^2))
cat("Stacked Model out-of-sample RMSE:", stacked_test_rmse, "\n")

```


### Modeling Choice

I chose to use the Stacked Model as my final predictive model. This is because the RMSEs of this model were extremely consistent across 5 different random seeds, and outperformed all previous models across 4 of 5 random seeds. It only slightly unperformed the Random Forest Model on one iteration. Although fairly consistent, the changes in RMSEs for the Random Forest and Gradient-boosted Tree Models could indicate a small amount of overfitting. Consequently, it seemed that by ensembling the three models I initially fit, I improved predictive accuracy and reduced the risk of overfitting. 


#### Partial Dependence of Green Certification

As my model of choice was the stacked model, I also calculated the partial dependence of 'green_certification' using a combination of the partial dependencies from the Linear Model, the Random Forest Model, and the Gradient-boosted Tree Model. I did this by first generating the partial dependencies of each model individually. I then weighted the respective partial dependencies by using the inverse of the RMSE and scaling those weights to that the sum to 1. In other words, the models with lower RMSE have more weight. 


```{r chunk23}

#Calculate partial dependencies

grid <- seq(from=0, to=1)

gbm_pd <- partial(green_boost, pred.var = "green_rating", grid.points = grid, train = green_test, n.trees=1000)

rf_pd <- partial(green_forest, pred.var = "green_rating", grid.points = grid, train = green_test, n.trees=500)

lm_coef <- coef(green_lm)["green_rating"]
lm_pd <- grid * lm_coef

```


```{r chunk24}

#Calculate inverse RMSEs for all models
gbm_inverse_rmse <- rmse_test_gbm ^ -1
rf_inverse_rmse <- rmse_test_rf ^ -1
lm_inverse_rmse <- rmse_test_lm ^ -1

total_inverse <- gbm_inverse_rmse + rf_inverse_rmse + lm_inverse_rmse

#Weight partial dependencies by inverse RMSE
gbm_weight <- gbm_inverse_rmse / total_inverse
rf_weight <- rf_inverse_rmse / total_inverse
lm_weight <- lm_inverse_rmse / total_inverse

#Calculate the Stacked Model Partial Dependency
stacked_pd <- gbm_weight*gbm_pd$yhat + rf_weight*rf_pd$yhat + lm_weight*lm_pd

```


```{r chunk25}
#Create Dataframe with the stacked partial dependency, makes easier to plot
df <- data.frame(grid,stacked_pd)
df <- df %>% rename(green_certification = grid)
df <- df %>% rename(revenue = stacked_pd)
df <- df %>% mutate(revenue = round(revenue, 3))
```

The following show the partial dependence of 'green_certification', and plots it. Note this shows that holding all other variables constant, having a 'green_certification' increases the 'revenue per square foot per calendar year' by about \$0.61.

```{r chunk26}
green_cert_pd <- df$revenue[2] - df$revenue[1]
cat("Partial Dependece of Green Certification:", green_cert_pd, "\n")
```

```{r chunk27}
#Plot Partial Dependency
ggplot(df, aes(x=green_certification, y=revenue)) + 
  geom_line() + 
  scale_y_continuous(
    limits = range(df$revenue),       
    breaks = range(df$revenue),      
    labels = range(df$revenue) 
  )
```


### Conclusion

The Stacked Model likely performed the best because it leverages the strengths of the other models and minimizes their weaknesses. For example, the Linear Model showed that holding all other variables constant, having a 'green_certification' increases the 'revenue per square foot per calendar year' by about \$1.32. Although this seems like a small number, it is actually ver significant. If a building has about 50 rooms averaging about 1,000 square feet, the difference in 'revenue' is approximately $66,000. There could be other factors not captured in the data that a linear model can account for that could contribute to this difference. In other words, we would need more data to train a better linear model as it is extremely difficult for a linear model to capture nuances based on the current dataset.

The Random Forest and Gradient-boosted Tree Models are better able to capture the nuances in the data. However, this is done at the risk of overfitting. Consequently, by creating a Stacked Model, the risk of overfitting is reduced. The partial dependence of the Stacked Model showed that holding all other variables constant, having a 'green_certification' increases the 'revenue per square foot per calendar year' by about \$0.57. If a building has about 50 rooms averaging about 1,000 square feet, the difference in 'revenue' is now approximately $28,500. 

Although much improved from the Linear Model, the RMSE of the Stacked Model is still pretty high at $7.205. More accurate predictions and a more accurate partial dependency of 'green_certification' can be achieved with more relevant data. This dataset contains data on commercial rental properties from across the United States, but does not even have the city and state in which these properties are located. Although the City_Market_Rent variable may capture some of this, even different locations within a city could have significantly different revenues. 

Overall, it seems like having a Green Certification does seem to have a positive effect on revenue, but based on the current amount of data, I cannot confidently conclude that it is a \$0.57 increase in 'revenue per square foot per calendar year' or that this relationship is causal.


## 4. Predictive Model Building: California Housing

```{r chunk28}
set.seed(9456)

#standardizing bedrooms and rooms by household
housing <- housing %>% mutate(avgBedrooms = totalBedrooms/households)
housing <- housing %>% mutate(avgRooms = totalRooms/households)

#train-test split
housing_split = initial_split(housing, prop = 0.8)
housing_train = training(housing_split)
housing_test = testing(housing_split)

```

### Methods

On this exercise, I generated 4 models: a KNN Model, a Random Forest Model, a GBM model, and a Stacked Model. I partitioned the dataset into a training set and a test set using an 80-20 split. For all models, the training set is used for training the model, hyperparameter tuning, and model selection. All K-fold cross validation was done on the training set; the test set was reserved for final validation of the models. 

#### KNN Model

The KNN Model was used as an initial Benchmark. The following show the RMSEs. The in-sample RMSE was generated using an average of 5 cross-validated RMSEs. The out-of-sample RMSE show the RMSE of predictions on the test set.

```{r chunk29}
#Scale the variables for KNN Training Set
housing_standardized_train <- housing_train %>% select(medianHouseValue, everything())

#Scale continuous variables (note response variable, price, and categorical variables are not scaled)
housing_standardized_train <- scale(housing_standardized_train[,2:11])

#Binds price, the scaled variables, and categorical variables back into a single dataframe
housing_standardized_train <- cbind(housing_train$medianHouseValue, housing_standardized_train)
colnames(housing_standardized_train)[1] <- "medianHouseValue"

housing_standardized_train <- data.frame(housing_standardized_train)
```

```{r chunk30}
#Scale the variables for KNN Test Set
housing_standardized_test <- housing_test %>% select(medianHouseValue, everything())

#Scale continuous variables (note response variable, price, and categorical variables are not scaled)
housing_standardized_test <- scale(housing_standardized_test[,2:11])

#Binds price, the scaled variables, and categorical variables back into a single dataframe
housing_standardized_test <- cbind(housing_test$medianHouseValue, housing_standardized_test)
colnames(housing_standardized_test)[1] <- "medianHouseValue"

housing_standardized_test <- data.frame(housing_standardized_test)
```

```{r chunk31}

set.seed(9456)

K_folds = 5
housing_standardized_folds <- housing_standardized_train %>%
  mutate(fold_id = rep(1:K_folds, length=nrow(housing_standardized_train)) %>% sample)

#Medium Model RMSE
rmse_knn = foreach(fold = 1:K_folds, .combine = 'c') %do% {
  knn = knnreg(medianHouseValue ~ . -totalRooms - totalBedrooms -housingMedianAge -avgRooms, 
                  data=filter(housing_standardized_folds, fold_id!=fold), k=9)
  modelr::rmse(knn, data = filter(housing_standardized_folds, fold_id == fold))
}

#KNN Model
housing_knn = knnreg(medianHouseValue ~ . -totalRooms - totalBedrooms -housingMedianAge -avgRooms, 
                  data=housing_standardized_train, k=9)

#RMSE
cat("KNN in-sample RMSE:", mean(rmse_knn), "\n")
cat("KNN out-of-sample RMSE:", rmse(housing_knn, housing_standardized_test), "\n")


```


#### Random Forest Model

The following show the RMSE results for the Random Forest Model. These show that there was significant improvement from the KNN Model, and serves as another benchmark to compare the GBM model to.

```{r chunk32}

set.seed(9456)

#Random Forest Model
housing_rf <- randomForest(medianHouseValue ~ . -totalRooms - totalBedrooms, 
                             data=housing_train, importance = TRUE, ntree=500)

#RMSE of 
rmse_train_rf <- modelr::rmse(housing_rf, housing_train)
rmse_test_rf <- modelr::rmse(housing_rf, housing_test)

cat("Random Forest Model in-sample RMSE:", rmse_train_rf, "\n")
cat("Random Forest Model out-of-sample RMSE:", rmse_test_rf, "\n")

```


#### GBM Model

The GBM Model was trained using 5 cross validated folds with an interaction depth of 10, a shrinkage of 0.1, and 1,000 trees. This model outperformed the Random Forest Model over 5 random seeds. The following show the RMSEs of the GBM Model.

```{r chunk33, include=FALSE}

set.seed(9456)

#Gradient-boosted Tree
housing_gbm = gbm(medianHouseValue ~ . -totalRooms - totalBedrooms, 
                  data=housing_train, interaction.depth=10, n.trees=1000, shrinkage=.1, distribution = 'gaussian', cv.folds=5)

rmse_train_gbm <- modelr::rmse(housing_gbm, housing_train)
rmse_test_gbm <- modelr::rmse(housing_gbm, housing_test)
```

```{r chunk34}
cat("GBM in-sample RMSE:", rmse_train_gbm, "\n")
cat("GBM out-of-sample RMSE:", rmse_test_gbm, "\n")
```



#### Stacked Model

Finally, I used a Stacked Model, in which I used an ensemble approach combining the predictions of the three separate GBM Models and used another GBM Model as a second level learner to stack the individual model predictions. Each GBM Model was generated by using a different random seed. Although the improvement on this model was relatively minor, this model outperformed the normal GBM Model over 5 random seeds. Consequently, this is my model of choice for generating the best predictive model. The following show the RMSEs of this model.

```{r chunk35}

#Create 2 additional gbm models

set.seed(1994)
#Gradient-boosted Tree
housing_gbm2 = gbm(medianHouseValue ~ . -totalRooms - totalBedrooms, 
                  data=housing_train, interaction.depth=10, n.trees=1000, shrinkage=.1, distribution = 'gaussian', cv.folds=5)

set.seed(1111)
#Gradient-boosted Tree
housing_gbm3 = gbm(medianHouseValue ~ . -totalRooms - totalBedrooms, 
                  data=housing_train, interaction.depth=10, n.trees=1000, shrinkage=.1, distribution = 'gaussian', cv.folds=5)

```


```{r chunk36, include=FALSE}

set.seed(9456)

#Generate Predictions on the training data
gbm_pred <- predict(housing_gbm, newdata = housing_train)
gbm_pred2 <- predict(housing_gbm2, newdata = housing_train)
gbm_pred3 <- predict(housing_gbm3, newdata = housing_train)

#Create a dataframe with predictions from all the models along with the medianHousePrice
stacked_data <- data.frame(gbm_pred, gbm_pred2, gbm_pred3, housing_train$medianHouseValue)
stacked_data <- stacked_data %>% rename(medianHouseValue = housing_train.medianHouseValue)

#Second level learner using gbm
stacked_model = gbm(medianHouseValue ~ ., 
                  data=stacked_data, interaction.depth=3, n.trees=1000, shrinkage=.01, distribution = 'gaussian', cv.folds=5)

#generate RMSE
stacked_train_rmse <- modelr::rmse(stacked_model, stacked_data)


```

```{r chunk37}
#print RMSE
cat("Stacked Model in-sample RMSE:", stacked_train_rmse, "\n")
```

```{r chunk38, include=FALSE}

#Generate predictions on test set
gbm_pred <- predict(housing_gbm, newdata = housing_test)
gbm_pred2 <- predict(housing_gbm2, newdata = housing_test)
gbm_pred3 <- predict(housing_gbm3, newdata = housing_test)

#Generate final prediction using stacked_model
stacked_data2 <- data.frame(gbm_pred, gbm_pred2, gbm_pred3, housing_test$medianHouseValue)
stacked_gbm_predictions <- predict(stacked_model, newdata = stacked_data2)
```

```{r chunk39}
stacked_test_rmse <- sqrt(mean((housing_test$medianHouseValue - stacked_gbm_predictions)^2))
cat("Stacked Model out-of-sample RMSE:", stacked_test_rmse, "\n")
```


#### Cross-Validated Predictions using the Stacked Model

Because I want to plot my predictions and residuals versus the geolocation, I generated cross-validated predictions using my stacked model. I first created 5 folds, using 4 folds as the training set and 1 fold as the test set. Using the training set, I created 3 separate GBM Models and passed them through a second-level learner (also using GBM) to generate predictions for the test set. I did this until each fold was used as a test set once so that out-of-sample predictions was generated for the whole dataset. 

```{r chunk40, include=FALSE}

K_folds = 5
housing_folds <- housing %>%
  mutate(fold_id = rep(1:K_folds, length=nrow(housing)) %>% sample)

#Create an empty dataframe with the same column names as the hotels_dev dataset
#I will use this to continue to append the out of sample predicted values into this dataset
df <- data.frame(matrix(ncol = ncol(housing), nrow = 0))
colnames(df) <- colnames(housing)

for (i in 1:K_folds) {
  
  #Create train test splits based on fold_id
  gbm_train <- housing_folds %>% filter(fold_id != i)
  gbm_test <- housing_folds %>% filter(fold_id == i)
  
  
  #Gradient-boosted Trees
  set.seed(9456)
  housing_gbm = gbm(medianHouseValue ~ . -totalRooms - totalBedrooms, 
                  data=gbm_train, interaction.depth=10, n.trees=1000, shrinkage=.1, distribution = 'gaussian', cv.folds=5)
  set.seed(1994)
  housing_gbm2 = gbm(medianHouseValue ~ . -totalRooms - totalBedrooms, 
                  data=gbm_train, interaction.depth=10, n.trees=1000, shrinkage=.1, distribution = 'gaussian', cv.folds=5)
  set.seed(1111)
  housing_gbm3 = gbm(medianHouseValue ~ . -totalRooms - totalBedrooms, 
                  data=gbm_train, interaction.depth=10, n.trees=1000, shrinkage=.1, distribution = 'gaussian', cv.folds=5)
  
  #Generate Predictions on the training data
  gbm_pred <- predict(housing_gbm, newdata = gbm_train)
  gbm_pred2 <- predict(housing_gbm2, newdata = gbm_train)
  gbm_pred3 <- predict(housing_gbm3, newdata = gbm_train)

  #Create a dataframe with predictions from all the models along with the medianHousePrice
  stacked_data <- data.frame(gbm_pred, gbm_pred2, gbm_pred3, gbm_train$medianHouseValue)
  stacked_data <- stacked_data %>% rename(medianHouseValue = gbm_train.medianHouseValue)

  #Second level learner using gbm
  stacked_model = gbm(medianHouseValue ~ ., 
                    data=stacked_data, interaction.depth=3, n.trees=1000, shrinkage=.01, distribution = 'gaussian', cv.folds=5)
  
  #Generate predictions on test set
  gbm_pred <- predict(housing_gbm, newdata = gbm_test)
  gbm_pred2 <- predict(housing_gbm2, newdata = gbm_test)
  gbm_pred3 <- predict(housing_gbm3, newdata = gbm_test)

  #Generate final prediction using stacked_model
  stacked_data2 <- data.frame(gbm_pred, gbm_pred2, gbm_pred3)
  stacked_gbm_predictions <- predict(stacked_model, newdata = stacked_data2)
  
  #Predict Values for test data/fold
  gbm_test$preds <- predict(stacked_model, newdata = stacked_data2)
  
  #Append the data with predictions to the empty dataframe
  df <- rbind(df, gbm_test)
}
```

The following shows the average RMSE of my predictions versus the actual medianHouseValues. This is the overall out-of-sample accuracy of my proposed model.

```{r chunk41}
final_rmse <- sqrt(mean((df$medianHouseValue - df$preds)^2))
cat("Stacked Model out-of-sample RMSE:", final_rmse, "\n")
```


#### Maps

```{r chunk42, include=FALSE}
#Load Libraries
library(mapview)

#Load Dataset
df <- read_csv("Data/ca.csv")
```

Plot using a color scale to show medianHouseValue versus longitude (x) and latitude (y).

```{r chunk43}
mapview(df, xcol = "longitude", ycol = "latitude", zcol= "medianHouseValue", crs = 4269, grid = FALSE)
```


Plot using a color scale to show predicted medianHouseValue versus longitude (x) and latitude (y).

```{r chunk44}
mapview(df, xcol = "longitude", ycol = "latitude", zcol= "preds", crs = 4269, grid = FALSE)
```

Plot using a color scale to show residuals versus longitude (x) and latitude (y).

```{r chunk45}
mapview(df, xcol = "longitude", ycol = "latitude", zcol= "residuals", crs = 4269, grid = FALSE)
```













